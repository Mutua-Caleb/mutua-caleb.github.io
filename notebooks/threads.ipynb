{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "import queue\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.parse\n",
    "import time\n",
    "import sqlite3\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter, defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import networkx as nx\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from urllib3.util.retry import Retry\n",
    "from requests.adapters import HTTPAdapter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "class WebCrawler:\n",
    "    def __init__(self, start_urls, max_depth=3, max_urls=100, num_threads=10, respect_robots=True, user_agent='PythonWebCrawler/1.0'):\n",
    "        self.start_urls = start_urls\n",
    "        self.max_depth = max_depth\n",
    "        self.max_urls = max_urls\n",
    "        self.num_threads = num_threads\n",
    "        self.respect_robots = respect_robots\n",
    "        self.user_agent = user_agent\n",
    "        self.queue = queue.PriorityQueue()\n",
    "        self.visited = set()\n",
    "        self.lock = threading.Lock()\n",
    "        self.url_count = 0\n",
    "        self.db_conn = sqlite3.connect('crawler_results.db', check_same_thread=False)\n",
    "        self.create_database()\n",
    "        self.robots_cache = {}\n",
    "        self.link_graph = nx.DiGraph()\n",
    "        self.session = requests.Session()\n",
    "        retries = Retry(total=5, backoff_factor=0.1, status_forcelist=[500, 502, 503, 504])\n",
    "        self.session.mount('http://', HTTPAdapter(max_retries=retries))\n",
    "        self.session.mount('https://', HTTPAdapter(max_retries=retries))\n",
    "\n",
    "    def create_database(self):\n",
    "        cursor = self.db_conn.cursor()\n",
    "        cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS pages (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            url TEXT UNIQUE,\n",
    "            title TEXT,\n",
    "            content TEXT,\n",
    "            links INTEGER,\n",
    "            depth INTEGER,\n",
    "            status_code INTEGER,\n",
    "            content_type TEXT,\n",
    "            crawl_time REAL,\n",
    "            keywords TEXT\n",
    "        )\n",
    "        ''')\n",
    "        self.db_conn.commit()\n",
    "\n",
    "\n",
    "    def save_to_database(self, url, title, content, links, depth, status_code, content_type, crawl_time, keywords):\n",
    "        cursor = self.db_conn.cursor()\n",
    "        cursor.execute('''\n",
    "        INSERT OR REPLACE INTO pages (url, title, content, links, depth, status_code, content_type, crawl_time, keywords)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (url, title, content, links, depth, status_code, content_type, crawl_time, json.dumps(keywords)))\n",
    "        self.db_conn.commit()\n",
    "\n",
    "    def crawl(self):\n",
    "        for url in self.start_urls:\n",
    "            self.queue.put((0, url, 0))  # (priority, url, depth)\n",
    "\n",
    "        futures = []\n",
    "        with ThreadPoolExecutor(max_workers=self.num_threads) as executor:\n",
    "            while not self.queue.empty() and self.url_count < self.max_urls:\n",
    "                _, url, depth = self.queue.get()\n",
    "                if url not in self.visited and depth <= self.max_depth:\n",
    "                    future = executor.submit(self.process_url, url, depth)\n",
    "                    futures.append(future)\n",
    "\n",
    "            for future in as_completed(futures):\n",
    "                future.result()  # This will raise any exceptions that occurred\n",
    "\n",
    "        logging.info(f\"Crawling complete. Processed {self.url_count} URLs.\")\n",
    "\n",
    "    def process_url(self, url, depth):\n",
    "        if url in self.visited:\n",
    "            return\n",
    "\n",
    "        with self.lock:\n",
    "            if self.url_count >= self.max_urls:\n",
    "                return\n",
    "            self.visited.add(url)\n",
    "            self.url_count += 1\n",
    "\n",
    "        try:\n",
    "            if self.respect_robots and not self.can_fetch(url):\n",
    "                logging.info(f\"Skipping {url} due to robots.txt restrictions\")\n",
    "                return\n",
    "\n",
    "            start_time = time.time()\n",
    "            response = self.session.get(url, timeout=10, headers={'User-Agent': self.user_agent})\n",
    "            content_type = response.headers.get('Content-Type', '').split(';')[0]\n",
    "\n",
    "            if 'text/html' not in content_type:\n",
    "                logging.info(f\"Skipping non-HTML content: {url} (Content-Type: {content_type})\")\n",
    "                return\n",
    "\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            title = soup.title.string if soup.title else \"No title\"\n",
    "            content = soup.get_text()\n",
    "            links = self.extract_links(soup, url)\n",
    "            keywords = self.extract_keywords(content)\n",
    "\n",
    "            crawl_time = time.time() - start_time\n",
    "\n",
    "            self.save_to_database(url, title, content[:1000], len(links), depth, response.status_code, content_type, crawl_time, keywords)\n",
    "\n",
    "            logging.info(f\"Processed: {url} (Depth: {depth}, Links: {len(links)}, Status: {response.status_code})\")\n",
    "\n",
    "            with self.lock:\n",
    "                for link in links:\n",
    "                    self.link_graph.add_edge(url, link)\n",
    "                    if link not in self.visited and depth < self.max_depth:\n",
    "                        priority = self.calculate_priority(link)\n",
    "                        self.queue.put((priority, link, depth + 1))\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "    def extract_links(self, soup, base_url):\n",
    "        links = []\n",
    "        for a_tag in soup.find_all('a', href=True):\n",
    "            href = a_tag['href']\n",
    "            full_url = urllib.parse.urljoin(base_url, href)\n",
    "            if full_url.startswith('http'):\n",
    "                links.append(full_url)\n",
    "        return links\n",
    "\n",
    "    def can_fetch(self, url):\n",
    "        parsed_url = urllib.parse.urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        if base_url not in self.robots_cache:\n",
    "            robots_url = urllib.parse.urljoin(base_url, \"/robots.txt\")\n",
    "            try:\n",
    "                response = self.session.get(robots_url, timeout=5)\n",
    "                if response.status_code == 200:\n",
    "                    self.robots_cache[base_url] = response.text\n",
    "                else:\n",
    "                    self.robots_cache[base_url] = \"\"\n",
    "            except:\n",
    "                self.robots_cache[base_url] = \"\"\n",
    "\n",
    "        robots_txt = self.robots_cache[base_url]\n",
    "        can_fetch = True\n",
    "        for line in robots_txt.split('\\n'):\n",
    "            if line.lower().startswith('user-agent:') and '*' in line:\n",
    "                can_fetch = True\n",
    "            elif line.lower().startswith('disallow:'):\n",
    "                disallow_path = line.split(':', 1)[1].strip()\n",
    "                if parsed_url.path.startswith(disallow_path):\n",
    "                    can_fetch = False\n",
    "                    break\n",
    "\n",
    "        return can_fetch\n",
    "\n",
    "    def extract_keywords(self, content):\n",
    "        words = word_tokenize(content.lower())\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "        return Counter(filtered_words).most_common(10)\n",
    "\n",
    "    def calculate_priority(self, url):\n",
    "        # Implement your own prioritization logic here\n",
    "        # For example, prioritize shorter URLs or specific domains\n",
    "        return len(url)\n",
    "\n",
    "class ResultAnalyzer:\n",
    "    def __init__(self, db_path, link_graph):\n",
    "        self.db_conn = sqlite3.connect(db_path)\n",
    "        self.link_graph = link_graph\n",
    "\n",
    "    def analyze(self):\n",
    "        self.basic_stats()\n",
    "        self.generate_visualizations()\n",
    "        self.analyze_content()\n",
    "        self.analyze_link_structure()\n",
    "        self.cluster_pages()\n",
    "\n",
    "    def basic_stats(self):\n",
    "        cursor = self.db_conn.cursor()\n",
    "        \n",
    "        # Most linked pages\n",
    "        cursor.execute(\"SELECT url, links FROM pages ORDER BY links DESC LIMIT 5\")\n",
    "        most_linked = cursor.fetchall()\n",
    "        logging.info(\"Top 5 most linked pages:\")\n",
    "        for url, links in most_linked:\n",
    "            logging.info(f\"{url}: {links} links\")\n",
    "\n",
    "        # Average links per page\n",
    "        cursor.execute(\"SELECT AVG(links) FROM pages\")\n",
    "        avg_links = cursor.fetchone()[0]\n",
    "        logging.info(f\"Average links per page: {avg_links:.2f}\")\n",
    "\n",
    "        # Pages by depth\n",
    "        cursor.execute(\"SELECT depth, COUNT(*) FROM pages GROUP BY depth\")\n",
    "        depth_distribution = cursor.fetchall()\n",
    "        logging.info(\"Pages by depth:\")\n",
    "        for depth, count in depth_distribution:\n",
    "            logging.info(f\"Depth {depth}: {count} pages\")\n",
    "\n",
    "        # Status code distribution\n",
    "        cursor.execute(\"SELECT status_code, COUNT(*) FROM pages GROUP BY status_code\")\n",
    "        status_distribution = cursor.fetchall()\n",
    "        logging.info(\"Status code distribution:\")\n",
    "        for status, count in status_distribution:\n",
    "            logging.info(f\"Status {status}: {count} pages\")\n",
    "\n",
    "        # Average crawl time\n",
    "        cursor.execute(\"SELECT AVG(crawl_time) FROM pages\")\n",
    "        avg_crawl_time = cursor.fetchone()[0]\n",
    "        logging.info(f\"Average crawl time per page: {avg_crawl_time:.3f} seconds\")\n",
    "\n",
    "    def generate_visualizations(self):\n",
    "        cursor = self.db_conn.cursor()\n",
    "\n",
    "        # Depth distribution\n",
    "        cursor.execute(\"SELECT depth, COUNT(*) FROM pages GROUP BY depth\")\n",
    "        depths, counts = zip(*cursor.fetchall())\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(depths, counts)\n",
    "        plt.title('Page Distribution by Depth')\n",
    "        plt.xlabel('Depth')\n",
    "        plt.ylabel('Number of Pages')\n",
    "        plt.savefig('depth_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Status code distribution\n",
    "        cursor.execute(\"SELECT status_code, COUNT(*) FROM pages GROUP BY status_code\")\n",
    "        status_codes, counts = zip(*cursor.fetchall())\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.bar(status_codes, counts)\n",
    "        plt.title('Page Distribution by Status Code')\n",
    "        plt.xlabel('Status Code')\n",
    "        plt.ylabel('Number of Pages')\n",
    "        plt.savefig('status_distribution.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Link graph visualization\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        pos = nx.spring_layout(self.link_graph)\n",
    "        nx.draw(self.link_graph, pos, node_size=10, node_color='blue', with_labels=False, arrows=True)\n",
    "        plt.title('Web Page Link Structure')\n",
    "        plt.savefig('link_graph.png')\n",
    "        plt.close()\n",
    "\n",
    "    def analyze_content(self):\n",
    "        cursor = self.db_conn.cursor()\n",
    "        cursor.execute(\"SELECT content FROM pages\")\n",
    "        all_content = ' '.join([row[0] for row in cursor.fetchall()])\n",
    "\n",
    "        # Tokenize and remove stop words\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words = word_tokenize(all_content.lower())\n",
    "        filtered_words = [word for word in words if word.isalnum() and word not in stop_words]\n",
    "\n",
    "        # Get most common words\n",
    "        word_freq = Counter(filtered_words)\n",
    "        most_common = word_freq.most_common(10)\n",
    "\n",
    "        logging.info(\"Most common words across all pages:\")\n",
    "        for word, count in most_common:\n",
    "            logging.info(f\"{word}: {count}\")\n",
    "\n",
    "        # Generate word cloud\n",
    "        try:\n",
    "            from wordcloud import WordCloud\n",
    "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_freq)\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.imshow(wordcloud, interpolation='bilinear')\n",
    "            plt.axis('off')\n",
    "            plt.title('Word Cloud of Crawled Content')\n",
    "            plt.savefig('word_cloud.png')\n",
    "            plt.close()\n",
    "        except ImportError:\n",
    "            logging.warning(\"WordCloud not installed. Skipping word cloud generation.\")\n",
    "\n",
    "    def analyze_link_structure(self):\n",
    "        # Calculate PageRank\n",
    "        pagerank = nx.pagerank(self.link_graph)\n",
    "        top_pages = sorted(pagerank.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        \n",
    "        logging.info(\"Top 10 pages by PageRank:\")\n",
    "        for url, rank in top_pages:\n",
    "            logging.info(f\"{url}: {rank:.4f}\")\n",
    "\n",
    "        # Identify strongly connected components\n",
    "        components = list(nx.strongly_connected_components(self.link_graph))\n",
    "        logging.info(f\"Number of strongly connected components: {len(components)}\")\n",
    "        logging.info(f\"Largest component size: {len(max(components, key=len))}\")\n",
    "\n",
    "    def cluster_pages(self):\n",
    "        cursor = self.db_conn.cursor()\n",
    "        cursor.execute(\"SELECT url, content FROM pages\")\n",
    "        pages = cursor.fetchall()\n",
    "\n",
    "        urls, contents = zip(*pages)\n",
    "        vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "        tfidf_matrix = vectorizer.fit_transform(contents)\n",
    "\n",
    "        num_clusters = min(5, len(pages))\n",
    "        kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "        kmeans.fit(tfidf_matrix)\n",
    "\n",
    "        clusters = defaultdict(list)\n",
    "        for url, cluster in zip(urls, kmeans.labels_):\n",
    "            clusters[cluster].append(url)\n",
    "\n",
    "        logging.info(\"Page clusters:\")\n",
    "        for cluster, urls in clusters.items():\n",
    "            logging.info(f\"Cluster {cluster}:\")\n",
    "            for url in urls[:5]:  # Show up to 5 URLs per cluster\n",
    "                logging.info(f\"  {url}\")\n",
    "            if len(urls) > 5:\n",
    "                logging.info(f\"  ... and {len(urls) - 5} more\")\n",
    "\n",
    "    def export_results(self, filename='crawler_results.json'):\n",
    "        cursor = self.db_conn.cursor()\n",
    "        cursor.execute(\"SELECT * FROM pages\")\n",
    "        columns = [description[0] for description in cursor.description]\n",
    "        results = [dict(zip(columns, row)) for row in cursor.fetchall()]\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        logging.info(f\"Results exported to {filename}\")\n",
    "\n",
    "    def close(self):\n",
    "        self.db_conn.close()\n",
    "\n",
    "def main():\n",
    "    start_time = time.time()\n",
    "\n",
    "    start_urls = [\n",
    "        \"https://en.wikipedia.org/wiki/Python_(programming_language)\",\n",
    "        \"https://www.python.org/\",\n",
    "        \"https://docs.python.org/3/\",\n",
    "    ]\n",
    "\n",
    "    crawler = WebCrawler(start_urls, max_depth=3, max_urls=100, num_threads=10, respect_robots=True)\n",
    "    crawler.crawl()\n",
    "\n",
    "    analyzer = ResultAnalyzer('crawler_results.db', crawler.link_graph)\n",
    "    analyzer.analyze()\n",
    "    analyzer.export_results()\n",
    "    analyzer.close()\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"Total execution time: {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
